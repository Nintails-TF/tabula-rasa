---
date created: Saturday, October 11th 2025, 10:07:40 am
date modified: Sunday, October 19th 2025, 3:25:16 pm
Parent Link: "[[../CDSA Index|CDSA Index]]"
---



# Security Monitoring and SIEM Fundamentals:

## Module Overview:
- [x] Understand core SIEM concepts and terminology
	- [x] Extensions notes and through understanding.
- [ ] Configure and navigate Elastic Stack 
	- [ ] Understand other potential tools other than Elastic stack.
- [ ] Define SOC roles and responsibilities 
- [ ] Apply MITRE ATT&CK framework to detection 
	- [ ] Understand other frameworks that exist and what countries use which.
- [ ] Develop and test SIEM use cases

## Summary:

***
# SIEM Fundamentals:

## What is SIEM? (Security Information and Event Management)

The purpose of SIEMs is that they collect information across an environment. In essence they and the first line of human intervention in the system, if preventative controls such as firewalls, patches, access controls do not work.

SIEMs are the eyes and ears of your security architecture, not the shield. They enable people to monitor a system in real-time and respond to threats and alerts within the system.

## What Do SIEMS Do?

The flow of a SIEM is as followed:
1. **Data ingestion or Collection** 
	1. SIEM solutions grab data from various sources.
2. **Data normalization and aggregation**
	1. Gathered data must be processed and normalized so that the SIEM can understand it.
3. **Data Analysis**
	1. SIEM displays data via dashboards, visualizations, alerts, and incidents so that a SOC team can identify potential risks and respond to them.

SIEM tools have great range of core functionality, including but not limited to:
- Collection and administration of log events.
- Examining log events and other supplemental data across the system.
- Incident handling.
- Visual summaries.
- Documentation.

SIEMs are used by IT personnel to detect cyberattacks that are in motion or respond to potential threats or errors that may pose problems in the future. It enables people to respond and resolve threats quicker.

SIEMs are the cornerstone of an organisations security tactics, giving people a complete working method for identifying, managing, and reporting on threats.
***
# The Evolution and History of SIEM:

SIEMs first came onto scene due to a partnership between two Garner analysts which combined Security Information Management (SIM) and Security Event Management (SEM). They [published this idea in 2005](https://www.tandfonline.com/doi/abs/10.1201/1086.1065898X/45390.14.3.20050701/89149.6) initially, since them SIEMs have evolved.

I've written a [[../Extension Notes/History of SIEMs|History of SIEMs]] that denotes the present and future of SIEMs, but to summarise:

**SIEM Evolution:**
- **1st Gen (2000s-2010s)**: On-premises, rule-based detection, compliance-focused log collection.
- **2nd Gen (2010s-present)**: Cloud-enabled, behavioural analytics, threat intelligence integration, SOAR automation, contextualized alerts.
- **3rd Gen (2025+)**: AI-driven with generative models, autonomous threat response, risk-based alerting, natural language interaction.
- **Core Evolution**: Passive log aggregation → Intelligent correlation → AI-assisted autonomous defence.
- **Major Barrier**: Regulated industries require explainable decisions; "black box" AI lacks auditability and trust.
***
# How Do SIEMs Work?:

## Basic Overview:

SIEMs systems work by gather data from many sources, such as: PCs, network devices, servers, and more. The data is then standardized and consolidated for easy analysis.

SIEMs generate huge amounts of alerts. An hourly log of a poorly-tuned SIEM can generate 100-1000 alerts. So fine-tuning of the SIEM to detect and alert high-risk events is critical.

SIEMs work in conjunction with IPS and IDS by processing the logs and other data generated by them to integrate data from various sources to map out, solve, and posture for threat detection and security issues.
## Technical Overview:

SIEMs differ from regular log-aggregation tools as they conduct various techniques to meet the demands of a security team or business.
### Parsing:

Parsing in a SIEM involves converting raw, unstructured logs into structured fields that can be searched, correlated, and analysed. This ensures logs from different sources share a consistent format through **normalization**, allowing the SIEM to effectively centralize and interpret data. 

The process typically includes **pattern matching** (using regex to identify log structures), **field extraction** (labelling values like user, IP, or event ID), and **type conversion** (transforming strings into usable data types such as integers or timestamps).

While tools like Splunk and Elastic Stack handle parsing differently, the core principles remain the same. SOC teams must manage challenges such as multi-line logs, inconsistent vendor formats, custom application outputs, and performance constraints—ensuring parsing remains accurate, efficient, and compliant with company or regulatory standards.

[[../Extension Notes/Parsing Process|The parsing process in detail.]]
### Enrichment:

Enrichment adds context to raw logs—turning “what happened” into “why it matters.” By combining data from threat intelligence, geolocation, and identity sources, it improves detection accuracy, accelerates investigations, and helps prioritize real threats.

However, enrichment must balance speed and accuracy; too many lookups or outdated feeds can cause delays and false positives. SOC teams should design efficient, regularly updated pipelines tailored to their environment to ensure enriched data drives faster, smarter security decisions.

[[../Extension Notes/Enrichment|The enrichment process in detail]]
### Indexing:

Indexing organizes parsed and enriched data so SIEMs can search massive datasets quickly using **inverted indexes** that map key values (like IPs or users) to specific events. By partitioning data by time, SIEMs scan only relevant logs, greatly improving search speed and efficiency.

Different SIEMs use unique methods—**Splunk** with TSIDX files and **Elastic** with Lucene-based indexes—but the goal is the same: enable fast, scalable, and cost-efficient querying. Effective indexing turns raw log storage into actionable intelligence within seconds.

[[../Extension Notes/Indexing|The indexing process in detail]]

### Retention Policies:

Retention policies tier log data by age to balance cost, performance, and compliance: **Hot** for real-time access, **Warm** for ongoing investigations, **Cold** for historical analysis, and **Frozen** for legal/archive needs. Tiering reduces storage costs compared to keeping all data on high-performance storage.

Policies must meet regulatory requirements (PCI DSS, HIPAA, GDPR, SOX, GLBA), consider log criticality and type, and ensure archived data can be restored. SIEMs like Splunk, Elastic, QRadar, and Sentinel provide lifecycle or archive settings to enforce these policies efficiently..

[[../Extension Notes/Retention Polices|Retention polices in detail]]

### Query Languages:

Query languages let analysts search, filter, and analyse SIEM data, with different SIEMs using different syntaxes. **Splunk SPL** uses a Unix-pipe style, ideal for stats and dashboards but proprietary and slower on large datasets. **KQL** (Elastic/Microsoft) is SQL-like, optimized for time-series queries, while **Lucene** supports quick ad-hoc searches in Elastic.

For efficiency, always use **indexed fields, time bounds, and early filters**. Avoid leading wildcards, full-index scans, heavy regex, or unnecessary field extraction to keep queries fast and scalable.

[[../Extension Notes/Query Languages|Query languages in detail]]
***
# The Architecture of SIEMs:

> [!TODO] Talk about the deployment of SIEMs

***
# What SIEMs Cannot Do:

> [!TODO] Explain the limits of current SIEMs

***
# SIEM Business Requirements & Use Cases:

> [!TODO] Revise and detail business cases in more detail

### Log Aggregation and Normalisation:

This is the most critical role of a SIEM - placing all the information from firewalls, applications, databases, etc. all into a single place for review.

It massively speeds up SOC teams so that they can react quicker and solve security incidents quicker.
### Threat Alerting:

Having a SIEM notify IT security teams about possible threats enables teams to carry out after action, more targeted investigations and respond to potential security issues efficiently.

By having the alarm bells sound quickly to the corresponding team, threats can be dealt with or minimized and business critical or key details can be secured.
### Contextualization & Response:

If a SIEM was to send an alert for every possible security event, a team will be flooded with alerts, it is key to contextualise alerts and determine who is involved what areas of the network are effected and what did it happen.

Automatic configurations can filter some threats that are contextualized. Since we want to reduce alert fatigue and concentrate on the most credible threats.
### Compliance:

SIEM solutions are mandatory due to government rules in certain organisations and industries. Regulations such as: PCI DSS, HIPAA, GDPR require that organisations have sufficient security.

SIEMs help gather data which is necessary for audits and regulators.

### Real-world Attack Scenarios:

## Why Do We Need SIEMs?

1. IT teams need a place where they can easily access all logs and data, especially for bigger organisations
2. IT teams can act more quickly in the even of security issues to protect or minimize the impact of security issues.
3. Banking, Finance, Insurance, Healthcare, and other critical industries require a managed SIEM on-premise or in the cloud to adhere to security and compliance standards such as ISO and HIPAA.

***
# Introduction into the Elastic Stack:

## What is the Elastic Stack?:

The Elastic stack, is an open-source collection that consists of three main applications working together (Elasticsearch, Logstash, and Kibana) with other components such as Kafka, RabbitMQ, Redis, and nginx layered on-top for resource-intensive environments

Most Elastic Solutions will look like the following:
![[../../../../../04_Reference/Pictures/elastic-stack-topography.png]]

An enterprise solution could be structured like so:
![[../../../../../04_Reference/Pictures/elastic-stack-enterprise-topography.png]]

***
## Components of the Elasticstack:

### Elasticsearch:

**Elasticsearch** is JSON-based searching engine, designed with RESTful APIs. The job of it is to handle indexing, storing and querying. **Elasticsearch** works in conjunction with **Logstash** to perform analytic operations on the log files processed by **Logstash.**
### Logstash:

**Logstash** is responsible for collecting, transforming, and transporting logs across the system. It's core strength is consolidating information from different sources and then normalising them. **Logstash** operates with three main contexts in mind:

1. Process Input
	1. **Logstash** ingests log file records from various remote sources and converts them into a uniform standard. It receiving logs through different [input plugins.](https://www.elastic.co/docs/reference/logstash/plugins/input-plugins) 
	2. An analyst can configure **Logstash** to grab details from many different sources as needed. Such as: *files, TCP sockets, syslogs, S3 buckets, etc.*
2. Transform and enrich log records
	1. Logstash offers numerous ways to modify log records to append, format, or modify a log record with additional data. You can see a list of these on their documentation of [Logstash.](https://www.elastic.co/docs/reference/logstash/plugins/filter-plugins)
	2. This can include: *age of an event, adding geographic details about an IP, looking up DNS, etc.*
3. Send logs to **Elasticsearch**
	1. Finally, now that input plugins and filter plugins have occurred, now **Logstash** uses [output plugins ](https://www.elastic.co/docs/reference/logstash/plugins/output-plugins)to transmit logs to Elasticsearch for further searching.

### Kibana:

**Kibana** serves as the visualisation tool for Elasticsearch documents. So that data can be viewed in Elasticsearch, then queries can be executed in **KQL (Kibana Query Language)** to filter for specific chunks or ranges of info.

Additionally, **Kibana** simplifies the dashboard into readable tables, charts, and custom dashboards.
### Beats:

**Beats** is another component frequently used in the **Elasticstack.** The purpose of it is to be a lightweight, single-purpose data shipper that is installed on remote machines or devices to forward logs and metrics to **Logstash** or **Elasticsearch** directly.

**Beats** simplifies the process of collecting data from various sources, meaning **Elasticstack** can get the information it needs without having to be pre-processed by **Logstash.**

So the information could flow like so:
`Beats` -> `Logstash` -> `Elasticsearch` -> `Kibana`
`Beats` -> `Elasticsearch` -> `Kibana`

***
# The Elastic Stack as a SIEM Solution:

The Elastic stack can be used as SIEM. So it can collect, store, analyse, and visually display security-related data from various sources.

For the elastic stack to work as a SIEM, security data from applications like firewalls, IDS/IPS, and endpoints should be ingested into the Elastic stack using Logstash first, then Elasticsearch should be configured to store and index the security data, finally Kibana should be used to build custom dashboards and visualisations into security-related events.

SOC analysts will spend most of their time working with Kibana as their primary tool when working with an Elastic Solution. So how does Kibana operate?:

## Diving into Kibana:

Our first tool in searching and using Kibana is **KQL (Kibana Query Language)**. KQL is a powerful tool that is also user-friendly which is designed for searching and analysing data in Kibana. It tends to be easier than Elasticsearch's Query DSL.

### Structure of KQL:

KQL queries are broken down into `field:value` pairs. The field attribute represents the attribute of the data whilst the value is the data that you are searching for. So in this example:

```Shell
event.code:4625

So:
Field = event.code
Value = 4625
```

What this means in practice, is that this filters for events that have the **Windows event code 4625** which stands for failed login attempts within Windows.

By using said query we can look at failed login attempts within Elasticsearch and investigate the source of the attempts and any potential security threats. Such as: password guessing, brute force attacks or other suspicious activities.

Queries can be further refined with additional conditions such as IP addresses, usernames, time range, etc. To zoom in specifically into potential security threats.

### Free Text Search:

KQL has a free-text search feature that allows you to search for a specific term across any field, without requiring a field name, such as:

```Shell
"svc-sql1"
```

Any record that contains the text "svc-sql1" will be returned

### Logical Operators:

KQL supports logical operators similar to SQL: AND, OR, NOT for example, this aids in building more complex queries, you use brackets for order of operations similar to mathematics. For example:

```Shell
event.code:4625 AND winlog.event_data.SubStatus:0xC0000072
```

I.e. Search for failed login (`event.code.4625`) attempts on any disabled accounts(`winlog.event_data.SubStatus:0xC0000072`) within a Windows System.

### Comparison Operators:

KQL also has comparison operators (e.g. `:, :>, :>=, :<, :<+, :!).` This can be helpful when checking ranges. For example:


```Shell
event.code:4625 AND winlog.event_data.SubStatus:0xC0000072 AND @timestamp >= "2023-03-03T00:00:00.000Z" AND @timestamp <= "2023-03-06T23:59:59.999Z"
```

Search for failed logins on disabled accounts between 03/03/2023 at 0:00 AM to 06/03/23 23:59 PM.

### Wildcards and Regular Expressions:

KQL supports wildcards and regular expressions if you need to search for patterns in fields, for example:

```Shell
event.code:4625 AND user.name: admin*
```

Search for failed logins for any account that starts with the username admin. (admin123, administrator, etc. would be search for.)

***

## How to Figure out the Available Fields and Values:

So the issue is now, what if you've never used a SOC before and you need to figure out what fields and values you can actually write queries in KQL for?

Within the [**Discover**](https://www.elastic.co/docs/explore-analyze/discover) tool in Kibana, we can sift through the data and by reading vendor documentation to understand what event codes or numbers mean. For example:

1. Windows event logs will denote that failed login code is 4625.
2. Use free-search for `4625` which will bring up `event.code:4625`, `winlog.event_id:4625` and `@timestamp`
	1. `event.code:4625` is related to the [Elastic Common Schema (ECS)](https://www.elastic.co/docs/reference/ecs/ecs-event#field-event-code)
	2. `winlog.event_id` is related to [Winlogbeat](https://www.elastic.co/docs/reference/beats/winlogbeat/exported-fields-winlog)

As it is dry, reading Elastic Documentation or interfacing with AI to explain these documentation notices, could be very helpful: 
- [Elastic Common Schema (ECS)](https://www.elastic.co/guide/en/ecs/current/ecs-reference.html)
- [Elastic Common Schema (ECS) event fields](https://www.elastic.co/guide/en/ecs/current/ecs-event.html)
- [Winlogbeat fields](https://www.elastic.co/guide/en/beats/winlogbeat/current/exported-fields-winlog.html)
- [Winlogbeat ECS fields](https://www.elastic.co/guide/en/beats/winlogbeat/current/exported-fields-ecs.html)
- [Winlogbeat security module fields](https://www.elastic.co/guide/en/beats/winlogbeat/current/exported-fields-security.html)
- [Filebeat fields](https://www.elastic.co/guide/en/beats/filebeat/current/exported-fields.html)
- [Filebeat ECS fields](https://www.elastic.co/guide/en/beats/filebeat/current/exported-fields-ecs.html)

### Elastic Common Schema (ECS):

The Elastic Common Schema (ECS) is a shared vocabulary list for events and logs across the elastic stack. This ensures that when using KQL (Kibana Query Language) within the Elastic stack that you have some more tools and features at your disposal.

- Unified Data View
	- *ECS enforces as structured and consistent approach to data.*
- Improved Search Efficiency
- Enhanced Correlation
- Better Visualisations
- Interoperability with Elastic Solutions
- Future-Proofing

***
## SOC Definition & Fundamentals:

## MITRE ATT&CK & Security Operations:

## SIEM Use Case Development:

***

# SIEM Visualization Development:

## SIEM Visualization Example 1: Failed Logon Attempts (All Users):

## SIEM Visualization Example 2: Failed Logon Attempts (Disabled Users):

## SIEM Visualization Example 3: Successful RDP Logon Related To Service Accounts:

## SIEM Visualization Example 4: Users Added Or Removed From A Local Group (Within A Specific Timeframe):

***

# The Triaging Process:

***

# Skills Assessment:

