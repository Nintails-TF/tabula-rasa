---
title: Crawling
updated: 2023-08-05T17:17:10.0000000+01:00
created: 2023-08-05T04:12:12.0000000+01:00
date modified: Monday, May 13th 2024, 10:19:32 pm
---

Introduction:

We can define **crawling** as the systematic or automatic process of exploring a website to list all of the resources we encounter. This reveals to us the structure of the website we are auditing and gives an overview of the attack surface.

We use crawling to find as many pages and subdirectories belonging to the website as possible.

ZAP:

A tool created by **OWASP** (Open Web Application Security Project) is **ZAP** (Zed Attack Proxy) it enables us to perform manual and automated security testing on web applications.

We can use it as a proxy server to intercept and manipulate traffic that passes through it. In this course, we are going to be using the **spidering** functionality.

1.  We open a browser configured to proxy in **Open ZAP.**
2.  We then write the website in the address bar and add it to scope.
3.  Now we can head to the Attack menu with a right-click and select **spider**
    1.  This can take some time, but once it is done we can see all the resources uncovered.

ZAP also has useful features such as a **built-in fuzzer** and a **manual request editor**. This means we can send any request to alter it and we can fuzz it with various payloads.

***ZAP** has [amazing documentation](https://www.zaproxy.org/docs/desktop/start/) so you can learn it quickly - it is also taught more in **Using Web Proxies.***

FFUF:

The ZAP spidering module only enumerates the resources it finds and in links and forms. *But it can miss hidden folders and files.*

We can use **ffuf** (Fuzz faster u fool) and recursively look through them.

When actually using ffuf within real engagements, we need to be careful to lower the rate of requests using the **-rate** parameter and the **-threads** parameters to avoid detection from IDS, WAFs and SIEMs.
Sensitive Data Disclosure:

**Usually,** webservers and web apps are able to handle the files they need to function. But we can also commonly find backup or unreferenced files that can have important information or creds.

Backups and unreferenced files can be generated by creating snapshots, different files versions and/or created by text editors without web developers knowledge.

*Commonly we can use **SecLists** and the **raft-\[ small \| medium \| large \]-extensions.txt** to look for hidden files.*

We can **extract keywords** from a website using **CeWL.** We can generate a wordlist using:
![image1](../../../../_resources/image1-159.png)

Where:
- *-m5*
  - *Minimum character length of words is 5.*
- *--lowercase*
  - *Converts words to lowercase.*
- *-w*
  - *The file to save the wordlist to.*

Then we can fuzz using multiple key words:

ffuf -w ./folders.txt:FOLDERS,./wordlist.txt:WORDLIST,./extensions.txt:EXTENSIONS -u <http://192.168.10.10/FOLDERS/WORDLISTEXTENSIONS>

![image2](../../../../_resources/image2-129.png)

